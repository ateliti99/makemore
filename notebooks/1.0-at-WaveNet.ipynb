{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WaveNet\n",
    "\n",
    "This notebook introduces an architecture similar to the WaveNet model developed by DeepMind in 2016. Before diving into the details of this model, we'll review the preliminary steps and implement them in a more comprehensive and modular manner. Drawing inspiration from PyTorch, here are the results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-23 15:40:03.843\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmakemore.config\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mPROJ_ROOT path is: /workspaces/makemore\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW_DATA_DIR: /workspaces/makemore/data/raw\n",
      "PROCESSED_DATA_DIR: /workspaces/makemore/data/processed\n"
     ]
    }
   ],
   "source": [
    "from makemore.config import RAW_DATA_DIR, PROCESSED_DATA_DIR\n",
    "print(f\"RAW_DATA_DIR: {RAW_DATA_DIR}\")\n",
    "print(f\"PROCESSED_DATA_DIR: {PROCESSED_DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-23 15:40:15.463\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmakemore.config\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mPROJ_ROOT path is: /workspaces/makemore\u001b[0m\n",
      "\u001b[32m2024-06-23 15:40:15.467\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m22\u001b[0m - \u001b[1mDownloading dataset...\u001b[0m\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  4.22it/s]\n",
      "\u001b[32m2024-06-23 15:40:15.707\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mnamex.txt correctly download!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Download the dataset\n",
    "!python ../makemore/dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['emma\\n', 'olivia\\n', 'ava\\n', 'isabella\\n', 'sophia\\n', 'charlotte\\n', 'mia\\n', 'amelia\\n', 'harper\\n', 'evelyn\\n']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'olivia',\n",
       " 'ava',\n",
       " 'isabella',\n",
       " 'sophia',\n",
       " 'charlotte',\n",
       " 'mia',\n",
       " 'amelia',\n",
       " 'harper',\n",
       " 'evelyn']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET_NAME = \"names.txt\"\n",
    "\n",
    "# Read words\n",
    "with open(RAW_DATA_DIR / DATASET_NAME, \"r\") as names_file:\n",
    "    names = names_file.readlines()\n",
    "\n",
    "print(names[:10])\n",
    "\n",
    "# Remove the \\n from all the names\n",
    "names = [name[:-1] for name in names]\n",
    "\n",
    "names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate decoder and encoder for chars\n",
    "chars = \".qwertyuiopasdfghjklzxcvbnm\" # All available letters in latin alphabet plus .\n",
    "decoder, encoder = {}, {}\n",
    "\n",
    "for i, ch in enumerate(chars):\n",
    "    encoder[ch] = i\n",
    "    decoder[i] = ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context window dimension\n",
    "context_size = 8\n",
    "vocab_size = len(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset lenght: 228145\n"
     ]
    }
   ],
   "source": [
    "# Store dataset\n",
    "X, Y = [], []\n",
    "\n",
    "for name in names:\n",
    "    # Starting context\n",
    "    context = [0] * context_size\n",
    "\n",
    "    for ch in name:\n",
    "        # Encode ch in an int\n",
    "        i_ch = encoder[ch]\n",
    "        # Store couple\n",
    "        X.append(context)\n",
    "        Y.append(i_ch)\n",
    "        # Update context\n",
    "        context = context[1:] + [i_ch]\n",
    "\n",
    "    i_ch = encoder[\".\"]\n",
    "    X.append(context)\n",
    "    Y.append(i_ch)\n",
    "\n",
    "# Store dataset in torch tensor\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n",
    "\n",
    "print(f\"Dataset lenght: {len(Y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorchifycantion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Layer ----------------------------------------------------------------\n",
    "class Linear:\n",
    "  \n",
    "  def __init__(self, fan_in, fan_out, bias=True):\n",
    "    self.weight = torch.randn((fan_in, fan_out)) / fan_in**0.5\n",
    "    self.bias = torch.zeros(fan_out) if bias else None\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    self.out = x @ self.weight\n",
    "    if self.bias is not None:\n",
    "      self.out += self.bias\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.weight] + ([] if self.bias is None else [self.bias])\n",
    "\n",
    "# BatchNorm Layer --------------------------------------------------------------\n",
    "class BatchNorm1d:\n",
    "  \n",
    "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "    self.eps = eps\n",
    "    self.momentum = momentum\n",
    "    self.training = True\n",
    "    # parameters (trained with backprop)\n",
    "    self.gamma = torch.ones(dim)\n",
    "    self.beta = torch.zeros(dim)\n",
    "    # buffers (trained with a running 'momentum update')\n",
    "    self.running_mean = torch.zeros(dim)\n",
    "    self.running_var = torch.ones(dim)\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    # calculate the forward pass\n",
    "    if self.training:\n",
    "      xmean = x.mean(0, keepdim=True) # batch mean\n",
    "      xvar = x.var(0, keepdim=True) # batch variance\n",
    "    else:\n",
    "      xmean = self.running_mean\n",
    "      xvar = self.running_var\n",
    "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "    self.out = self.gamma * xhat + self.beta\n",
    "    # update the buffers\n",
    "    if self.training:\n",
    "      with torch.no_grad():\n",
    "        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
    "        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.gamma, self.beta]\n",
    "\n",
    "# Tanh Layer -----------------------------------------------------------------------\n",
    "class Tanh:\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    self.out = torch.tanh(x)\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return []\n",
    "  \n",
    "# Embedding Layer ------------------------------------------------------------------\n",
    "class Embedding:\n",
    "  \n",
    "  def __init__(self, num_embeddings, embedding_dim):\n",
    "    self.weight = torch.randn((num_embeddings, embedding_dim))\n",
    "    \n",
    "  def __call__(self, IX):\n",
    "    self.out = self.weight[IX]\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.weight]\n",
    "  \n",
    "# Flatten Layer --------------------------------------------------------------------\n",
    "class Flatten:\n",
    "  \n",
    "  def __init__(self):\n",
    "    return\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    return x.view(x.shape[0], -1)\n",
    "  \n",
    "  def parameters(self):\n",
    "    return []\n",
    "\n",
    "# Sequential Container -------------------------------------------------------------\n",
    "class Sequential:\n",
    "  \n",
    "  def __init__(self, layers):\n",
    "    self.layers = layers\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    for layer in self.layers:\n",
    "      x = layer(x)\n",
    "    self.out = x\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    # get parameters of all layers and stretch them out into one list\n",
    "    return [p for layer in self.layers for p in layer.parameters()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7600dafc23f0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set a manual seed\n",
    "torch.manual_seed(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Embedding.__init__() missing 2 required positional arguments: 'num_embeddings' and 'embedding_dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m context_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m      5\u001b[0m g \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mGenerator()\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m2147483647\u001b[39m) \u001b[38;5;66;03m# for reproducibility\u001b[39;00m\n\u001b[1;32m      7\u001b[0m layers \u001b[38;5;241m=\u001b[39m Sequential([\n\u001b[0;32m----> 8\u001b[0m   \u001b[43mEmbedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, Flatten(),\n\u001b[1;32m      9\u001b[0m   Linear(n_embd \u001b[38;5;241m*\u001b[39m context_size, n_hidden, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m), BatchNorm1d(n_hidden), Tanh(),\n\u001b[1;32m     10\u001b[0m   Linear(n_hidden, vocab_size, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m), BatchNorm1d(vocab_size),\n\u001b[1;32m     11\u001b[0m ])\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     14\u001b[0m   \u001b[38;5;66;03m# last layer: make less confident\u001b[39;00m\n\u001b[1;32m     15\u001b[0m   layers[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: Embedding.__init__() missing 2 required positional arguments: 'num_embeddings' and 'embedding_dim'"
     ]
    }
   ],
   "source": [
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 100 # the number of neurons in the hidden layer of the MLP\n",
    "vocab_size = 27\n",
    "context_size = 3\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "\n",
    "layers = Sequential([\n",
    "  Embedding(), Flatten(),\n",
    "  Linear(n_embd * context_size, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  Linear(n_hidden, vocab_size, bias=False), BatchNorm1d(vocab_size),\n",
    "])\n",
    "\n",
    "with torch.no_grad():\n",
    "  # last layer: make less confident\n",
    "  layers[-1].gamma *= 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6224\n"
     ]
    }
   ],
   "source": [
    "parameters = layers.parameters()\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
